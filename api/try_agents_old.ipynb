{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory,ConversationSummaryBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.agents.types import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "import requests\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "credential_path = f\"../.credential\"\n",
    "# check is credential_path exist\n",
    "assert os.path.exists(credential_path) is True\n",
    "load_dotenv(credential_path)\n",
    "\n",
    "def get_chromadb_setting():\n",
    "    setting = chromadb.config.Settings()\n",
    "    setting.persist_directory = f\"../../vector_db/chroma\"\n",
    "    return setting\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# conversational_memory = ConversationBufferWindowMemory(\n",
    "#     memory_key=\"chat_history\", return_messages=True\n",
    "# )\n",
    "gpt_model = \"gpt-3.5-turbo\"\n",
    "max_tokens = 500\n",
    "conversational_memory = ConversationSummaryBufferMemory(\n",
    "        llm=ChatOpenAI(temperature=0.0, max_tokens=max_tokens, model_name=gpt_model),\n",
    "        # input_key=\"question\",\n",
    "        # output_key=\"answer\",\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        max_token_limit=1000,\n",
    "        # chat_memory=loaded_chat_memory,\n",
    "        # chat_memory=[],\n",
    "    )\n",
    "def load_db_api(\n",
    "    collection_name,\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    ") -> VectorStore:\n",
    "    db = None\n",
    "    # try:\n",
    "    #     url = f\"http://localhost:8000/api/v1/collections/{collection_name}\"\n",
    "    #     response = requests.get(url)\n",
    "    #     assert response.status_code == 200\n",
    "    #     db = Chroma(\n",
    "    #         collection_name=collection_name,\n",
    "    #         client=chromadb.HttpClient(),\n",
    "    #         embedding_function=embeddings,\n",
    "    #     )\n",
    "\n",
    "    #     print(\"\\tLoad chroma index success\")\n",
    "    # except Exception:\n",
    "    #     raise LoadDBException(\"Load chroma index failed\")\n",
    "    # url = f\"http://localhost:8000/api/v1/collections/{collection_name}\"\n",
    "    print(\"\\n\\there,\")\n",
    "    try:\n",
    "        url = f\"http://localhost:8000/api/v1/collections/{collection_name}\"\n",
    "        response = requests.get(url)\n",
    "        print(\"\\n\\there,\", response.status_code, url)\n",
    "    except:\n",
    "        pass\n",
    "    # try:\n",
    "    #     url = f\"http://chroma_server:8000/api/v1/collections/{collection_name}\"\n",
    "    #     response = requests.get(url)\n",
    "    #     print(\"\\n\\there,\", response.status_code, url)\n",
    "    # except:\n",
    "    #     pass\n",
    "    # try:\n",
    "    #     url = f\"http://server:8000/api/v1/collections/{collection_name}\"\n",
    "    #     response = requests.get(url)\n",
    "    #     print(\"\\n\\there,\", response.status_code, url)\n",
    "    # except:\n",
    "    #     pass\n",
    "    assert response.status_code == 200\n",
    "\n",
    "    db = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        client=chromadb.HttpClient(\n",
    "            host=\"localhost\", settings=get_chromadb_setting()\n",
    "        ),\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "\n",
    "    print(\"\\tLoad chroma index success\")\n",
    "    return db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\there,\n",
      "\n",
      "\there, 200 http://localhost:8000/api/v1/collections/product_a_small\n",
      "\tLoad chroma index success\n"
     ]
    }
   ],
   "source": [
    "db = load_db_api(\"product_a_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.agents import OpenAIFunctionsAgent\n",
    "system_message = SystemMessage(content=\"You are very powerful assistant, but bad at calculating lengths of words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=system_message,\n",
    "    extra_prompt_messages=[MessagesPlaceholder(variable_name=MEMORY_KEY)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "tools = [get_word_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=system_message,\n",
    "    extra_prompt_messages=[MessagesPlaceholder(variable_name=MEMORY_KEY)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=MEMORY_KEY, return_messages=True)\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_word_length` with `{'word': 'educa'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m5\u001b[0m\u001b[32;1m\u001b[1;3mThere are 5 letters in the word \"educa\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNo, \"educa\" is not a real word in English.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No, \"educa\" is not a real word in English.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "\n",
    "agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "agent_executor.run(\"how many letters in the word educa?\")\n",
    "agent_executor.run(\"is that a real word?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.agents.agent.AgentExecutor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(agent_executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "class NoOpLLMChain(LLMChain):\n",
    "    \"\"\"No-op LLM chain.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__(\n",
    "            llm=ChatOpenAI(), prompt=PromptTemplate(template=\"\", input_variables=[])\n",
    "        )\n",
    "\n",
    "    def run(self, question: str, *args, **kwargs) -> str:\n",
    "        return question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory, ChatMessageHistory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "        llm=ChatOpenAI(temperature=0.0, max_tokens=max_tokens, model_name=gpt_model),\n",
    "        input_key=\"question\",\n",
    "        output_key=\"answer\",\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        max_token_limit=1000,\n",
    "        chat_memory=ChatMessageHistory(),\n",
    "    )\n",
    "other_qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(temperature=0.0, max_tokens=max_tokens, model_name=gpt_model),\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory,\n",
    "    retriever=db.as_retriever(),\n",
    "    return_generated_question=True,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "no_op_chain = NoOpLLMChain()\n",
    "other_qa.question_generator = no_op_chain\n",
    "modified_template = \"Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\\nChat History:\\n{chat_history}\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(modified_template)\n",
    "other_qa.combine_docs_chain.llm_chain.prompt.messages[0] = system_message_prompt\n",
    "\n",
    "# add chat_history as a variable to the llm_chain's ChatPromptTemplate object\n",
    "other_qa.combine_docs_chain.llm_chain.prompt.input_variables = [\n",
    "    \"context\",\n",
    "    \"question\",\n",
    "    \"chat_history\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'callbacks': None,\n",
       " 'callback_manager': None,\n",
       " 'verbose': False,\n",
       " 'tags': None,\n",
       " 'metadata': None,\n",
       " 'prompt': ChatPromptTemplate(input_variables=['context', 'question', 'chat_history'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'context'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\\nChat History:\\n{chat_history}\", template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={})]),\n",
       " 'llm': ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=500, tiktoken_model_name=None),\n",
       " 'output_key': 'text',\n",
       " 'output_parser': StrOutputParser(),\n",
       " 'return_final_only': True,\n",
       " 'llm_kwargs': {}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_qa.combine_docs_chain.llm_chain.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory,ConversationTokenBufferMemory\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(),\n",
    ")\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#         llm=ChatOpenAI(temperature=0.0, max_tokens=max_tokens, model_name=gpt_model),\n",
    "#         input_key=\"question\",\n",
    "#         output_key=\"answer\",\n",
    "#         memory_key=\"chat_history\",\n",
    "#         return_messages=True,\n",
    "#         max_token_limit=1000,\n",
    "#         chat_memory=ChatMessageHistory(),\n",
    "#     )\n",
    "# qa = ConversationalRetrievalChain.from_llm(\n",
    "#     llm=ChatOpenAI(temperature=0.0, max_tokens=max_tokens, model_name=gpt_model),\n",
    "#     chain_type=\"stuff\",\n",
    "#     memory=memory,\n",
    "#     retriever=db.as_retriever(),\n",
    "#     return_generated_question=True,\n",
    "#     return_source_documents=True,\n",
    "# )\n",
    "\n",
    "# no_op_chain = NoOpLLMChain()\n",
    "# qa.question_generator = no_op_chain\n",
    "# modified_template = \"Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\\nChat History:\\n{chat_history}\"\n",
    "# system_message_prompt = SystemMessagePromptTemplate.from_template(modified_template)\n",
    "# qa.combine_docs_chain.llm_chain.prompt.messages[0] = system_message_prompt\n",
    "\n",
    "# # add chat_history as a variable to the llm_chain's ChatPromptTemplate object\n",
    "# qa.combine_docs_chain.llm_chain.prompt.input_variables = [\n",
    "#     \"context\",\n",
    "#     \"question\",\n",
    "#     \"chat_history\",\n",
    "# ]\n",
    "\n",
    "# system_message = \"\"\"\n",
    "#         You are a Product information chatbot name naxon( first letter n from neuron and axon from axon). \n",
    "#         You can ask questions to help you understand user intent.\n",
    "#         You should only talk within the context of problem.\n",
    "#         If you are unsure of how to help, you can suggest the client to contact the customer support.\n",
    "#         You should talk on Thai, unless the client talks in English. \"\"\"\n",
    "system_message = \"\"\"\n",
    "        You are a Product information chatbot name naxon( first letter n from neuron and axon from axon). \n",
    "        You can ask questions to help you understand user intent.\n",
    "        You should only talk within the context of problem.\n",
    "        If you are unsure of how to help, you can suggest the client to contact the customer support.\n",
    "        You should talk on Thai, unless the client talks in English. \n",
    "        ####\n",
    "        Chat History:\\n{chat_history}\"\n",
    "        \"\"\"\n",
    "# modified_template = \"Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\\nChat History:\\n{chat_history}\"\n",
    "# system_message_prompt = SystemMessagePromptTemplate.from_template(modified_template)\n",
    "\n",
    "# qa.combine_documents_chain.llm_chain.prompt.messages[0] = modified_template\n",
    "# qa.combine_documents_chain.llm_chain.prompt.input_variables = [\n",
    "#         \"context\",\n",
    "#         \"question\",\n",
    "#         \"chat_history\",\n",
    "#     ]\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"qa-product-information\",\n",
    "        func=qa.run,\n",
    "        description=\"Useful when you need to information about the product.\",\n",
    "    )\n",
    "]\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "memory = ConversationTokenBufferMemory(llm=llm,memory_key=MEMORY_KEY,max_token_limit=1000, return_messages=True)\n",
    "\n",
    "executor = initialize_agent(\n",
    "    agent = AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    # memory=conversational_memory,\n",
    "    max_iterations=3,\n",
    "    agent_kwargs={\"system_message\": system_message},\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'callbacks', 'callback_manager', 'verbose', 'tags', 'metadata', 'prompt', 'llm', 'output_key', 'output_parser', 'return_final_only', 'llm_kwargs'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.__dict__['agent'].__dict__['llm_chain'].__dict__.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t memory\n",
      "\t callbacks\n",
      "\t callback_manager\n",
      "\t verbose\n",
      "\t tags\n",
      "\t metadata\n",
      "\t prompt\n",
      "other_qa_dict\t input_variables=['context', 'question', 'chat_history'] output_parser=None partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'context'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\\nChat History:\\n{chat_history}\", template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={})]\n",
      "qa_dict\t\t input_variables=['input', 'chat_history', 'agent_scratchpad'] output_parser=None partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history'], output_parser=None, partial_variables={}, template='\\n        You are a Product information chatbot name naxon( first letter n from neuron and axon from axon). \\n        You can ask questions to help you understand user intent.\\n        You should only talk within the context of problem.\\n        If you are unsure of how to help, you can suggest the client to contact the customer support.\\n        You should talk on Thai, unless the client talks in English. \\n        ####\\n        Chat History:\\n{chat_history}\"\\n        ', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> qa-product-information: Useful when you need to information about the product.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\ The action to take. Must be one of qa-product-information\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')]\n",
      "\n",
      "\t llm\n",
      "other_qa_dict\t cache=None verbose=False callbacks=None callback_manager=None tags=None metadata=None client=<class 'openai.api_resources.chat_completion.ChatCompletion'> model_name='gpt-3.5-turbo' temperature=0.0 model_kwargs={} openai_api_key='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' openai_api_base='' openai_organization='' openai_proxy='' request_timeout=None max_retries=6 streaming=False n=1 max_tokens=500 tiktoken_model_name=None\n",
      "qa_dict\t\t cache=None verbose=False callbacks=None callback_manager=None tags=None metadata=None client=<class 'openai.api_resources.chat_completion.ChatCompletion'> model_name='gpt-3.5-turbo' temperature=0.0 model_kwargs={} openai_api_key='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' openai_api_base='' openai_organization='' openai_proxy='' request_timeout=None max_retries=6 streaming=False n=1 max_tokens=None tiktoken_model_name=None\n",
      "\n",
      "\t output_key\n",
      "\t output_parser\n",
      "\t return_final_only\n",
      "\t llm_kwargs\n"
     ]
    }
   ],
   "source": [
    "other_qa_dict = other_qa.combine_docs_chain.llm_chain.__dict__\n",
    "qa_dict = executor.__dict__['agent'].__dict__['llm_chain'].__dict__\n",
    "\n",
    "for key in other_qa_dict.keys():\n",
    "    print(\"\\t\",key)\n",
    "    if other_qa_dict[key] != qa_dict[key]:\n",
    "        print(\"other_qa_dict\\t\",other_qa_dict[key])\n",
    "        print(\"qa_dict\\t\\t\",qa_dict[key])\n",
    "\n",
    "        print()\n",
    "for key in other_qa_dict.keys():\n",
    "    if key !=\"prompt\":\n",
    "        continue\n",
    "    qa_prompt = qa_dict[key]\n",
    "    other_qa_prompt = other_qa_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_variables': ['input', 'chat_history', 'agent_scratchpad'],\n",
       " 'output_parser': None,\n",
       " 'partial_variables': {},\n",
       " 'messages': [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history'], output_parser=None, partial_variables={}, template='\\n        You are a Product information chatbot name naxon( first letter n from neuron and axon from axon). \\n        You can ask questions to help you understand user intent.\\n        You should only talk within the context of problem.\\n        If you are unsure of how to help, you can suggest the client to contact the customer support.\\n        You should talk on Thai, unless the client talks in English. \\n        ####\\n        Chat History:\\n{chat_history}\"\\n        ', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       "  MessagesPlaceholder(variable_name='chat_history'),\n",
       "  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> qa-product-information: Useful when you need to information about the product.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\ The action to take. Must be one of qa-product-information\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       "  MessagesPlaceholder(variable_name='agent_scratchpad')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_prompt.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_variables': ['context', 'question', 'chat_history'],\n",
       " 'output_parser': None,\n",
       " 'partial_variables': {},\n",
       " 'messages': [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'context'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\\nChat History:\\n{chat_history}\", template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       "  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={})]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_qa_prompt.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.agents import OpenAIFunctionsAgent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.agents import OpenAIFunctionsAgent\n",
    "system_message = SystemMessage(content=\"You are very powerful assistant, but bad at calculating lengths of words.\")\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=system_message,\n",
    "    extra_prompt_messages=[MessagesPlaceholder(variable_name=MEMORY_KEY)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "tools = [get_word_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory,ConversationTokenBufferMemory\n",
    "# memory = ConversationTokenBufferMemory(llm=llm,memory_key=MEMORY_KEY,max_token_limit=1000, return_messages=True)\n",
    "memory = ConversationBufferMemory(memory_key=MEMORY_KEY, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"get_word_length\",\n",
      "    \"action_input\": \"educa\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m5\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"5\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"get_word_length\",\n",
      "    \"action_input\": \"educa\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m5\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Yes, 'educa' is a real word.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes, 'educa' is a real word.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import ConversationalChatAgent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "# agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent = ConversationalChatAgent.from_llm_and_tools(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "agent_executor.run(\"how many letters in the word educa?\")\n",
    "agent_executor.run(\"is that a real word?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input'], output_parser=None, partial_variables={}, messages=[SystemMessage(content='You are a Product information chatbot name naxon( first letter n from neuron and axon from axon). \\n        You can ask questions to help you understand user intent.\\n        You should only talk within the context of problem.\\n        If you are unsure of how to help, you can suggest the client to contact the customer support.\\n        You should talk on Thai, unless the client talks in English. \\n        ', additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='{input}', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message_content = \"\"\"You are a Product information chatbot name naxon( first letter n from neuron and axon from axon). \n",
    "        You can ask questions to help you understand user intent.\n",
    "        You should only talk within the context of problem.\n",
    "        If you are unsure of how to help, you can suggest the client to contact the customer support.\n",
    "        You should talk on Thai, unless the client talks in English. \n",
    "        \"\"\"\n",
    "system_message = SystemMessage(content=system_message_content)\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=system_message,\n",
    "    extra_prompt_messages=[MessagesPlaceholder(variable_name=MEMORY_KEY)]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from typing import Optional, Type\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "\n",
    "\n",
    "class CalculatorInput(BaseModel):\n",
    "    question: str = Field()\n",
    "\n",
    "class CustomCalculatorTool(BaseTool):\n",
    "    name = \"Calculator\"\n",
    "    description = \"useful for when you need to answer questions about math\"\n",
    "    args_schema: Type[BaseModel] = CalculatorInput\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return llm_math_chain.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m qa \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(\n\u001b[1;32m      2\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[1;32m      3\u001b[0m     chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     retriever\u001b[39m=\u001b[39mdb\u001b[39m.\u001b[39mas_retriever(),\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m tools \u001b[39m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     Tool(\n\u001b[1;32m      8\u001b[0m         name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mqa-product-information\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m         func\u001b[39m=\u001b[39;49mqa\u001b[39m.\u001b[39;49mrun,\n\u001b[1;32m     10\u001b[0m         description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mUseful when you need to information about the product.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m---> 11\u001b[0m     )\u001b[39m.\u001b[39;49margs(\u001b[39m\"\u001b[39;49m\u001b[39mchat_history\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m \u001b[39m# agent = ConversationalChatAgent.from_llm_and_tools(llm=llm, tools=tools, prompt=prompt)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m agent \u001b[39m=\u001b[39m OpenAIFunctionsAgent(llm\u001b[39m=\u001b[39mllm, tools\u001b[39m=\u001b[39mtools, prompt\u001b[39m=\u001b[39mprompt)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(),\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"qa-product-information\",\n",
    "        func=qa.run,\n",
    "        description=\"Useful when you need to information about the product.\",\n",
    "    ).args(\"chat_history\")\n",
    "]\n",
    "# agent = ConversationalChatAgent.from_llm_and_tools(llm=llm, tools=tools, prompt=prompt)\n",
    "agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `qa-product-information` with `washing machine`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'chat_history'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwhat is the dimension of washing machine?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m executor\u001b[39m.\u001b[39;49mrun(prompt)\n\u001b[1;32m      3\u001b[0m \u001b[39m# executor.run({\"input\":prompt,\"chat_history\":chat_history})\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:475\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    474\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    476\u001b[0m         _output_key\n\u001b[1;32m    477\u001b[0m     ]\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    481\u001b[0m         _output_key\n\u001b[1;32m    482\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:282\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    283\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    284\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:276\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    270\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    271\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    277\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    278\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/agents/agent.py:1036\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1036\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m   1037\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1038\u001b[0m         color_mapping,\n\u001b[1;32m   1039\u001b[0m         inputs,\n\u001b[1;32m   1040\u001b[0m         intermediate_steps,\n\u001b[1;32m   1041\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1042\u001b[0m     )\n\u001b[1;32m   1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1044\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1045\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1046\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/agents/agent.py:891\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    889\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    892\u001b[0m         agent_action\u001b[39m.\u001b[39;49mtool_input,\n\u001b[1;32m    893\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    894\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[1;32m    895\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    896\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_run_kwargs,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/tools/base.py:351\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    350\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    352\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[1;32m    354\u001b[0m         \u001b[39mstr\u001b[39m(observation), color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    355\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/tools/base.py:323\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     tool_args, tool_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    322\u001b[0m     observation \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 323\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49mtool_args, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_kwargs)\n\u001b[1;32m    324\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    325\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39mtool_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs)\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m \u001b[39mexcept\u001b[39;00m ToolException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    328\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_tool_error:\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/tools/base.py:493\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[1;32m    491\u001b[0m new_argument_supported \u001b[39m=\u001b[39m signature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\n\u001b[1;32m    494\u001b[0m         \u001b[39m*\u001b[39;49margs,\n\u001b[1;32m    495\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    497\u001b[0m     )\n\u001b[1;32m    498\u001b[0m     \u001b[39mif\u001b[39;00m new_argument_supported\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    500\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:475\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    474\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    476\u001b[0m         _output_key\n\u001b[1;32m    477\u001b[0m     ]\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    481\u001b[0m         _output_key\n\u001b[1;32m    482\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:282\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    283\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    284\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:276\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    270\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    271\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    277\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    278\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:139\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs(question)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_documents_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    140\u001b[0m     input_documents\u001b[39m=\u001b[39;49mdocs, question\u001b[39m=\u001b[39;49mquestion, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child()\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_source_documents:\n\u001b[1;32m    144\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: answer, \u001b[39m\"\u001b[39m\u001b[39msource_documents\u001b[39m\u001b[39m\"\u001b[39m: docs}\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:480\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    476\u001b[0m         _output_key\n\u001b[1;32m    477\u001b[0m     ]\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 480\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    481\u001b[0m         _output_key\n\u001b[1;32m    482\u001b[0m     ]\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    485\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    486\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:259\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    225\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    226\u001b[0m     inputs: Union[Dict[\u001b[39mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m     include_run_info: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    233\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    234\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39m            `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_inputs(inputs)\n\u001b[1;32m    260\u001b[0m     callback_manager \u001b[39m=\u001b[39m CallbackManager\u001b[39m.\u001b[39mconfigure(\n\u001b[1;32m    261\u001b[0m         callbacks,\n\u001b[1;32m    262\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    269\u001b[0m     new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:413\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     external_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mload_memory_variables(inputs)\n\u001b[1;32m    412\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexternal_context)\n\u001b[0;32m--> 413\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_inputs(inputs)\n\u001b[1;32m    414\u001b[0m \u001b[39mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:171\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    169\u001b[0m missing_keys \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_keys)\u001b[39m.\u001b[39mdifference(inputs)\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m missing_keys:\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing some input keys: \u001b[39m\u001b[39m{\u001b[39;00mmissing_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'chat_history'}"
     ]
    }
   ],
   "source": [
    "prompt = \"what is the dimension of washing machine?\"\n",
    "executor.run(prompt)\n",
    "# executor.run({\"input\":prompt,\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': []}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.memory.__dict__['chat_memory'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"qa-product-information\",\n",
      "    \"action_input\": \"what did I just say?\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mYou did not say anything in the provided context.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I apologize, but I am not able to remember or retrieve previous tool responses. Is there anything else I can assist you with?\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I apologize, but I am not able to remember or retrieve previous tool responses. Is there anything else I can assist you with?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"what did I just say?\"\n",
    "executor.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the dimension of washing machine?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The dimensions of the washing machine are 848 x 596.5 x 575 mm (for models EWF8024P5WB and EWF8024P5SB) and 848 x 596.5 x 659 mm (for model EWF9024P5WB).', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='what is the dimension of washing machine?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The dimensions of the washing machine are 848 x 596.5 x 575 mm (for models EWF8024P5WB and EWF8024P5SB) and 848 x 596.5 x 659 mm (for model EWF9024P5WB).', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='what did I just say?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='I apologize, but I am not able to remember or retrieve previous tool responses. Is there anything else I can assist you with?', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.memory.__dict__['chat_memory'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='\\n        You are a Product information chatbot name naxon( first letter n from neuron and axon from axon). \\n        You can ask questions to help you understand user intent.\\n        You should only talk within the context of problem.\\n        If you are unsure of how to help, you can suggest the client to contact the customer support.\\n        You should talk on Thai, unless the client talks in English.\\n        ', template_format='f-string', validate_template=True),\n",
       " 'additional_kwargs': {}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.__dict__['agent'].__dict__['llm_chain'].__dict__['prompt'].__dict__['messages'][0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' !  Naxon ?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\n",
    "try:\n",
    "    response= executor.run(prompt)\n",
    "except Exception as e:\n",
    "        response = str(e)\n",
    "        if response.startswith(\"Could not parse LLM output:\"):\n",
    "            response = response.removeprefix(\"Could not parse LLM output:\").removesuffix(\"`\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Hi Ford! How can I assist you today?\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi Ford! How can I assist you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"hi my name is ford\"\n",
    "try:\n",
    "    response= executor.run(prompt)\n",
    "except Exception as e:\n",
    "        response = str(e)\n",
    "        if response.startswith(\"Could not parse LLM output:\"):\n",
    "            response = response.removeprefix(\"Could not parse LLM output:\").removesuffix(\"`\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, but as an AI chatbot, I don't have access to personal information. Therefore, I don't know your name. Is there anything else I can assist you with?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"what is my name?\"\n",
    "try:\n",
    "    response= executor.run(prompt)\n",
    "except Exception as e:\n",
    "        response = str(e)\n",
    "        if response.startswith(\"Could not parse LLM output:\"):\n",
    "            response = response.removeprefix(\"Could not parse LLM output:\").removesuffix(\"`\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
