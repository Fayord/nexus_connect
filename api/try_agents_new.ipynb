{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory,ConversationSummaryBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.agents.types import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "import requests\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "credential_path = f\"../.credential\"\n",
    "# check is credential_path exist\n",
    "assert os.path.exists(credential_path) is True\n",
    "load_dotenv(credential_path)\n",
    "\n",
    "def get_chromadb_setting():\n",
    "    setting = chromadb.config.Settings()\n",
    "    setting.persist_directory = f\"../../vector_db/chroma\"\n",
    "    return setting\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# conversational_memory = ConversationBufferWindowMemory(\n",
    "#     memory_key=\"chat_history\", return_messages=True\n",
    "# )\n",
    "gpt_model = \"gpt-3.5-turbo\"\n",
    "max_tokens = 500\n",
    "#conversational_memory = ConversationBufferWindowMemory( memory_key='chat_history', k=50, return_messages=True, input_key=\"question\" )\n",
    "conversational_memory = ConversationSummaryBufferMemory(\n",
    "        llm=ChatOpenAI(temperature=0.0, max_tokens=max_tokens, model_name=gpt_model),\n",
    "        input_key=\"question\",\n",
    "        # output_key=\"answer\",\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        max_token_limit=1000,\n",
    "        # chat_memory=loaded_chat_memory,\n",
    "        # chat_memory=[],\n",
    "    )\n",
    "def load_db_api(\n",
    "    collection_name,\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    ") -> VectorStore:\n",
    "    db = None\n",
    "    # try:\n",
    "    #     url = f\"http://localhost:8000/api/v1/collections/{collection_name}\"\n",
    "    #     response = requests.get(url)\n",
    "    #     assert response.status_code == 200\n",
    "    #     db = Chroma(\n",
    "    #         collection_name=collection_name,\n",
    "    #         client=chromadb.HttpClient(),\n",
    "    #         embedding_function=embeddings,\n",
    "    #     )\n",
    "\n",
    "    #     print(\"\\tLoad chroma index success\")\n",
    "    # except Exception:\n",
    "    #     raise LoadDBException(\"Load chroma index failed\")\n",
    "    # url = f\"http://localhost:8000/api/v1/collections/{collection_name}\"\n",
    "    print(\"\\n\\there,\")\n",
    "    try:\n",
    "        url = f\"http://localhost:8000/api/v1/collections/{collection_name}\"\n",
    "        response = requests.get(url)\n",
    "        print(\"\\n\\there,\", response.status_code, url)\n",
    "    except:\n",
    "        pass\n",
    "    # try:\n",
    "    #     url = f\"http://chroma_server:8000/api/v1/collections/{collection_name}\"\n",
    "    #     response = requests.get(url)\n",
    "    #     print(\"\\n\\there,\", response.status_code, url)\n",
    "    # except:\n",
    "    #     pass\n",
    "    # try:\n",
    "    #     url = f\"http://server:8000/api/v1/collections/{collection_name}\"\n",
    "    #     response = requests.get(url)\n",
    "    #     print(\"\\n\\there,\", response.status_code, url)\n",
    "    # except:\n",
    "    #     pass\n",
    "    assert response.status_code == 200\n",
    "\n",
    "    db = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        client=chromadb.HttpClient(\n",
    "            host=\"localhost\", settings=get_chromadb_setting()\n",
    "        ),\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "\n",
    "    print(\"\\tLoad chroma index success\")\n",
    "    return db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\there,\n",
      "\n",
      "\there, 200 http://localhost:8000/api/v1/collections/tv_sony\n",
      "\tLoad chroma index success\n"
     ]
    }
   ],
   "source": [
    "# db = load_db_api(\"washing_machine_haier\")\n",
    "db = load_db_api(\"tv_sony\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory, ChatMessageHistory\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.memory.chat_memory import BaseChatMemory\n",
    "class NoOpLLMChain(LLMChain):\n",
    "    \"\"\"No-op LLM chain.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__(\n",
    "            llm=ChatOpenAI(), prompt=PromptTemplate(template=\"\", input_variables=[])\n",
    "        )\n",
    "\n",
    "    def run(self, question: str, *args, **kwargs) -> str:\n",
    "        return question\n",
    "\n",
    "\n",
    "def create_qa(\n",
    "    db: VectorStore,\n",
    "    memory: BaseChatMemory,\n",
    "    db_params: dict = {\"search_type\": \"mmr\", \"search_kwargs\": {\"k\": 3}},\n",
    ") -> ConversationalRetrievalChain:\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(temperature=0.0, max_tokens=max_tokens, model_name=gpt_model),\n",
    "        chain_type=\"stuff\",\n",
    "        memory=memory,\n",
    "        retriever=db.as_retriever(**db_params),\n",
    "        # return_generated_question=True,\n",
    "        # return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    no_op_chain = NoOpLLMChain()\n",
    "    qa.question_generator = no_op_chain\n",
    "    modified_template = \"Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\\nChat History:\\n{chat_history}\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(modified_template)\n",
    "    qa.combine_docs_chain.llm_chain.prompt.messages[0] = system_message_prompt\n",
    "\n",
    "    # add chat_history as a variable to the llm_chain's ChatPromptTemplate object\n",
    "    qa.combine_docs_chain.llm_chain.prompt.input_variables = [\n",
    "        \"context\",\n",
    "        \"question\",\n",
    "        \"chat_history\",\n",
    "    ]\n",
    "    return qa\n",
    "\n",
    "# chat_history_qa = create_qa(db, conversational_memory)\n",
    "from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "\n",
    "readonlymemory = ReadOnlySharedMemory(memory=conversational_memory)\n",
    "chat_history_qa = create_qa(db, readonlymemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VectorStore.as_retriever() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 36\u001b[0m\n\u001b[1;32m     16\u001b[0m template \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mYou are a Product information chatbot name naxon\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[39m \u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39mYou have the following products in your store based on the customer question. \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mQuestion: \u001b[39m\u001b[39m{question}\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mAnswer:\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     31\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate(template\u001b[39m=\u001b[39mtemplate, input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     33\u001b[0m qa \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(\n\u001b[1;32m     34\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[1;32m     35\u001b[0m     chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m---> 36\u001b[0m     retriever\u001b[39m=\u001b[39mdb\u001b[39m.\u001b[39;49mas_retriever({ \u001b[39m\"\u001b[39;49m\u001b[39msearch_kwargs\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\u001b[39m\"\u001b[39;49m\u001b[39mk\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m2\u001b[39;49m}}),\n\u001b[1;32m     37\u001b[0m     chain_type_kwargs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: prompt, \u001b[39m\"\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mmemory\u001b[39m\u001b[39m\"\u001b[39m: readonlymemory},\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m system_message \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[39m        You are a Product information chatbot name naxon( first letter n from neuron and axon from axon). \u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[39m        You can ask questions to help you understand user intent.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m        You should talk in Thai, unless the client talks in English. \u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     48\u001b[0m tools \u001b[39m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m     Tool(\n\u001b[1;32m     50\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mqa-product-information\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: VectorStore.as_retriever() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.agents.types import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0,verbose=True)\n",
    "\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True,\n",
    ")\n",
    "template = \"\"\"You are a Product information chatbot name naxon\n",
    " \n",
    "You have the following products in your store based on the customer question. \n",
    "Talk politely professional.\n",
    "You can ask questions to help you understand user intent.\n",
    "You should only talk within the context of problem.\n",
    "If you are unsure of how to help, you can suggest the client to contact the customer support.\n",
    "You should talk in Thai, unless the client talks in English. \n",
    "\n",
    "{context}\n",
    "\n",
    "\"chat_history\": {chat_history}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\", \"chat_history\"])\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever({ \"search_kwargs\": {\"k\": 2}}),\n",
    "    chain_type_kwargs = {\"prompt\": prompt, \"verbose\": True, \"memory\": readonlymemory},\n",
    "\n",
    ")\n",
    "\n",
    "system_message = \"\"\"\n",
    "        You are a Product information chatbot name naxon( first letter n from neuron and axon from axon). \n",
    "        You can ask questions to help you understand user intent.\n",
    "        You should only talk within the context of problem.\n",
    "        If you are unsure of how to help, you can suggest the client to contact the customer support.\n",
    "        You should talk in Thai, unless the client talks in English. \n",
    "        \"\"\"\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"qa-product-information\",\n",
    "        func=qa.run,\n",
    "        # func=chat_history_qa.run,\n",
    "        description=\"Useful when you need to information about the product.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "executor = initialize_agent(\n",
    "    agent = AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    memory=conversational_memory,\n",
    "    agent_kwargs={\"system_message\": system_message},\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "template = \"\"\"You are a Product information chatbot name naxon\n",
    " \n",
    "You have the following products in your store based on the customer question. \n",
    "Talk politely professional.\n",
    "You can ask questions to help you understand user intent.\n",
    "You should only talk within the context of problem.\n",
    "If you are unsure of how to help, you can suggest the client to contact the customer support.\n",
    "You should talk in Thai, unless the client talks in English. \n",
    "\n",
    "{context}\n",
    "\n",
    "\"chat_history\": {chat_history}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\", \"chat_history\"])\n",
    "\n",
    "product_inquery_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    chain_type_kwargs = {\"prompt\": prompt, \"verbose\": True, \"memory\": readonlymemory},\n",
    ")\n",
    "\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Product inquiries System\",\n",
    "        func=product_inquery_chain.run,\n",
    "        # description=\"useful for getting information about products, features, and specifications to make informed purchase decisions. Input should be a fully formed question.\",\n",
    "        description=\"Useful when you need to information about the product.\",\n",
    "        return_direct=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from langchain import     OpenAI\n",
    "\n",
    "\n",
    "prefix = \"\"\"Have a conversation with a human as a customer representative agent, answering the following questions as best you can in a polite and friendly manner. You have access to the following tools:\"\"\"\n",
    "suffix = \"\"\"Begin!\" {chat_history} Question: {question} {agent_scratchpad}\"\"\"\n",
    "\n",
    "\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools, \n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"question\", \"chat_history\", \"agent_scratchpad\"], \n",
    "    )\n",
    "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\n",
    "\n",
    "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
    "zerop_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=conversational_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"qa-product-information\",\n",
      "    \"action_input\": \"HDMI\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a Product information chatbot name naxon\n",
      " \n",
      "You have the following products in your store based on the customer question. \n",
      "Talk politely professional.\n",
      "You can ask questions to help you understand user intent.\n",
      "You should only talk within the context of problem.\n",
      "If you are unsure of how to help, you can suggest the client to contact the customer support.\n",
      "You should talk on Thai, unless the client talks in English. \n",
      "\n",
      "คมชัดระดับ 4K HDR ให้ภาพสมจริงขึ้นไปอีกระดับ ให้สีสัน ความมืด และความสว่างที่ถูกต้องระบบประมวลผลภาพ 4K X-Reality PRO เพิ่มความคมชัด และ 4K Resolution เพื่อเพิ่มความละเอียดของภาพชิปประมวลผล 4K processor X1 ที่ทำให้ภาพมีคุณภาพและมิติระบบปฏิบัติการ Google TV สามารถออกคำสั่งทีวีด้วยเสียงภาษาไทยโดยไม่จำเป็นต้องใช้รีโมทสามารถเชื่อมเนื้อหาบนโทรศัพท์มือถือทั้ง Android และ IOS ในการรองรับ Chrome cast และ Apple Airplayสั่งงานเครื่องใช้ไฟฟ้าอื่นๆ ในบ้านที่รองรับระบบ IOT โดยผ่าน Apple Homekitจัดระเบียบสายต่าง ๆ ที่เชื่อมต่อกับทีวีให้ดูเป็นระเบียบสวยงามด้วย Cable managementความจุภายใน 16 GB (พื้นที่บางส่วนอาจถูกใช้งานเพื่อข้อมูลในส่วนของระบบปฏิบัติการและแอปพลิเคชั่น)กำลังเสียงของลำโพง 10W+10W และ รองรับ Digital TV (DVB –T2)\n",
      "\n",
      "- ความสูง TV รวมขาตั้ง: 70.6 ซม.\n",
      "- การรับประกัน: 3 ปี\n",
      "รายละเอียดสินค้า: แอลอีดีทีวี 50\" SONY (4K, LED, Google TV) KD-50X75K\n",
      "ในวันพักผ่อนอยู่บ้านสบาย ๆ จะมีอะไรที่ดีไปกว่าการมีทีวีตอบโจทย์ทุกความต้องการของคุณอย่าง LED TV รุ่น KD-50X75K ขนาด 50 นิ้ว จาก SONY ที่ให้ภาพคมชัดระดับ 4K HDR ขับเคลื่อนโดยชิปประมวลผล 4K HDR Processor X1 ที่มอบภาพที่มีคอนทราสต์สมจริง ให้ความมืด ความสว่างที่ถูกต้องตรงปก พร้อมชิปประมวลผล 4K processor X1 เพื่อให้ภาพมีมิติ และเพิ่มความสนุกอีกขั้นด้วยการรองรับระบบปฏิบัติการ Google TV, Chorome cast และ Apple Airplay โดยสามารถเชื่อมต่อกับมือถือและแทบเล็ตได้อย่างง่ายดาย สามารถเรียกดูภาพยนตร์และรายการทีวีจากสตรีมมิ่งของคุณทั้งหมดในที่เดียว\n",
      "\n",
      "- แบรนด์: SONY\n",
      "- ซีรีส์: X75\n",
      "- ความสูง: 66.2 ซม.\n",
      "- ความกว้าง: 112.7 ซม.\n",
      "- ความลึก: 7.7 ซม.\n",
      "- น้ำหนัก: 9.8 กก.\n",
      "- ขนาดหน้าจอ: 50 นิ้ว\n",
      "- ความละเอียดหน้าจอ: 4K (2160P)\n",
      "- ชนิดหน้าจอ: LED\n",
      "- DIGITAL TV BUILT IN: YES\n",
      "- SMART TV: ANDROID TV\n",
      "- HDR FORMAT: HDR10\n",
      "- HDMI ports (ช่อง): 3\n",
      "- USB ports (ช่อง): 2\n",
      "- ประเภทรีโมท: NORMAL REMOTE\n",
      "- ความลึก TV รวมขาตั้ง: 23.1 ซม.\n",
      "- ความสูง TV รวมขาตั้ง: 70.6 ซม.\n",
      "- การรับประกัน: 3 ปี\n",
      "รายละเอียดสินค้า: แอลอีดีทีวี 50\" SONY (4K, LED, Google TV) KD-50X75K\n",
      "\n",
      "ราคาเริ่มต้น: 19,990 บาท\n",
      "ราคาลด/ราคาโปรโมชั่น: 17,540 บาท\n",
      "ขนาด: 55\"\n",
      "ขนาด: 65\"\n",
      "ขนาด: 50\"\n",
      "ขนาด: 43\"\n",
      "คำอธิบายสินค้า: คมชัดระดับ 4K HDR ให้ภาพสมจริงขึ้นไปอีกระดับ ให้สีสัน ความมืด และความสว่างที่ถูกต้อง\n",
      "คำอธิบายสินค้า: ระบบประมวลผลภาพ 4K X-Reality PRO เพิ่มความคมชัด และ 4K Resolution เพื่อเพิ่มความละเอียดของภาพ\n",
      "คำอธิบายสินค้า: ชิปประมวลผล 4K processor X1 ที่ทำให้ภาพมีคุณภาพและมิติ\n",
      "\n",
      "\"chat_history\": [HumanMessage(content='What capacity washing machine have?', additional_kwargs={}, example=False), AIMessage(content='The capacity of the HAIER HW150-BP14986ES9 front-loading washing machine is 15 kg. It can handle the washing needs of the whole family, including bedding, large stuffed toys, and favorite clothes. The larger drum size and wider lid make it convenient to load and unload. It also offers a variety of washing programs to suit your needs. Additionally, it features new technologies that help eliminate germs and bacteria, such as high-temperature washing programs and odor reduction technology. It ensures cleanliness and safety for your loved ones.', additional_kwargs={}, example=False)]\n",
      "\n",
      "Question: HDMI\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe SONY KD-50X75K TV has 3 HDMI ports. HDMI ports allow you to connect external devices such as gaming consoles, Blu-ray players, and soundbars to your TV for a better viewing experience.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The SONY KD-50X75K TV has 3 HDMI ports.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "response: The SONY KD-50X75K TV has 3 HDMI ports.\n",
      "Total Tokens: 2955\n",
      "Prompt Tokens: 2858\n",
      "Completion Tokens: 97\n",
      "Total Cost (USD): $0.004481\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "\n",
    "    # message = \"What capacity washing machine have?\"\n",
    "    # message = \"มีความจุเท่าไหร่ฦ\"\n",
    "    # message = \"นายชื่อว่าอะไร\"\n",
    "    message = \"มีHDMI กี่ช่อง\"\n",
    "    response = executor.run(message)\n",
    "    # response = executor(message)\n",
    "    print(\"response:\", response)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"qa-product-information\",\n",
      "    \"action_input\": \"HDMI ports\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a Product information chatbot name naxon\n",
      " \n",
      "You have the following products in your store based on the customer question. \n",
      "Talk politely professional.\n",
      "You can ask questions to help you understand user intent.\n",
      "You should only talk within the context of problem.\n",
      "If you are unsure of how to help, you can suggest the client to contact the customer support.\n",
      "You should talk on Thai, unless the client talks in English. \n",
      "\n",
      "คมชัดระดับ 4K HDR ให้ภาพสมจริงขึ้นไปอีกระดับ ให้สีสัน ความมืด และความสว่างที่ถูกต้องระบบประมวลผลภาพ 4K X-Reality PRO เพิ่มความคมชัด และ 4K Resolution เพื่อเพิ่มความละเอียดของภาพชิปประมวลผล 4K processor X1 ที่ทำให้ภาพมีคุณภาพและมิติระบบปฏิบัติการ Google TV สามารถออกคำสั่งทีวีด้วยเสียงภาษาไทยโดยไม่จำเป็นต้องใช้รีโมทสามารถเชื่อมเนื้อหาบนโทรศัพท์มือถือทั้ง Android และ IOS ในการรองรับ Chrome cast และ Apple Airplayสั่งงานเครื่องใช้ไฟฟ้าอื่นๆ ในบ้านที่รองรับระบบ IOT โดยผ่าน Apple Homekitจัดระเบียบสายต่าง ๆ ที่เชื่อมต่อกับทีวีให้ดูเป็นระเบียบสวยงามด้วย Cable managementความจุภายใน 16 GB (พื้นที่บางส่วนอาจถูกใช้งานเพื่อข้อมูลในส่วนของระบบปฏิบัติการและแอปพลิเคชั่น)กำลังเสียงของลำโพง 10W+10W และ รองรับ Digital TV (DVB –T2)\n",
      "\n",
      "- แบรนด์: SONY\n",
      "- ซีรีส์: X75\n",
      "- ความสูง: 66.2 ซม.\n",
      "- ความกว้าง: 112.7 ซม.\n",
      "- ความลึก: 7.7 ซม.\n",
      "- น้ำหนัก: 9.8 กก.\n",
      "- ขนาดหน้าจอ: 50 นิ้ว\n",
      "- ความละเอียดหน้าจอ: 4K (2160P)\n",
      "- ชนิดหน้าจอ: LED\n",
      "- DIGITAL TV BUILT IN: YES\n",
      "- SMART TV: ANDROID TV\n",
      "- HDR FORMAT: HDR10\n",
      "- HDMI ports (ช่อง): 3\n",
      "- USB ports (ช่อง): 2\n",
      "- ประเภทรีโมท: NORMAL REMOTE\n",
      "- ความลึก TV รวมขาตั้ง: 23.1 ซม.\n",
      "- ความสูง TV รวมขาตั้ง: 70.6 ซม.\n",
      "- การรับประกัน: 3 ปี\n",
      "รายละเอียดสินค้า: แอลอีดีทีวี 50\" SONY (4K, LED, Google TV) KD-50X75K\n",
      "\n",
      "- ความสูง TV รวมขาตั้ง: 70.6 ซม.\n",
      "- การรับประกัน: 3 ปี\n",
      "รายละเอียดสินค้า: แอลอีดีทีวี 50\" SONY (4K, LED, Google TV) KD-50X75K\n",
      "ในวันพักผ่อนอยู่บ้านสบาย ๆ จะมีอะไรที่ดีไปกว่าการมีทีวีตอบโจทย์ทุกความต้องการของคุณอย่าง LED TV รุ่น KD-50X75K ขนาด 50 นิ้ว จาก SONY ที่ให้ภาพคมชัดระดับ 4K HDR ขับเคลื่อนโดยชิปประมวลผล 4K HDR Processor X1 ที่มอบภาพที่มีคอนทราสต์สมจริง ให้ความมืด ความสว่างที่ถูกต้องตรงปก พร้อมชิปประมวลผล 4K processor X1 เพื่อให้ภาพมีมิติ และเพิ่มความสนุกอีกขั้นด้วยการรองรับระบบปฏิบัติการ Google TV, Chorome cast และ Apple Airplay โดยสามารถเชื่อมต่อกับมือถือและแทบเล็ตได้อย่างง่ายดาย สามารถเรียกดูภาพยนตร์และรายการทีวีจากสตรีมมิ่งของคุณทั้งหมดในที่เดียว\n",
      "\n",
      "ราคาเริ่มต้น: 19,990 บาท\n",
      "ราคาลด/ราคาโปรโมชั่น: 17,540 บาท\n",
      "ขนาด: 55\"\n",
      "ขนาด: 65\"\n",
      "ขนาด: 50\"\n",
      "ขนาด: 43\"\n",
      "คำอธิบายสินค้า: คมชัดระดับ 4K HDR ให้ภาพสมจริงขึ้นไปอีกระดับ ให้สีสัน ความมืด และความสว่างที่ถูกต้อง\n",
      "คำอธิบายสินค้า: ระบบประมวลผลภาพ 4K X-Reality PRO เพิ่มความคมชัด และ 4K Resolution เพื่อเพิ่มความละเอียดของภาพ\n",
      "คำอธิบายสินค้า: ชิปประมวลผล 4K processor X1 ที่ทำให้ภาพมีคุณภาพและมิติ\n",
      "\n",
      "\"chat_history\": [HumanMessage(content='What capacity washing machine have?', additional_kwargs={}, example=False), AIMessage(content='The capacity of the HAIER HW150-BP14986ES9 front-loading washing machine is 15 kg. It can handle the washing needs of the whole family, including bedding, large stuffed toys, and favorite clothes. The larger drum size and wider lid make it convenient to load and unload. It also offers a variety of washing programs to suit your needs. Additionally, it features new technologies that help eliminate germs and bacteria, such as high-temperature washing programs and odor reduction technology. It ensures cleanliness and safety for your loved ones.', additional_kwargs={}, example=False)]\n",
      "\n",
      "Question: HDMI ports\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe SONY KD-50X75K LED TV has 3 HDMI ports.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The SONY KD-50X75K LED TV has 3 HDMI ports.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "response: The SONY KD-50X75K LED TV has 3 HDMI ports.\n",
      "Total Tokens: 2971\n",
      "Prompt Tokens: 2900\n",
      "Completion Tokens: 71\n",
      "Total Cost (USD): $0.004492\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "\n",
    "    # message = \"What capacity washing machine have?\"\n",
    "    # message = \"มีความจุเท่าไหร่ฦ\"\n",
    "    message = \"มี HDMI กี่ช่อง\"\n",
    "    # message = \"How many HDMI ports?\"\n",
    "\n",
    "    # message = \"เมื่อกี้ตอบว่าอะไร\"\n",
    "    try:\n",
    "        response = executor.run(message)\n",
    "    except Exception as e:\n",
    "        # print(f\"Error: {e}\")\n",
    "        response = f\"{e}\"\n",
    "    \n",
    "    # response = executor(message)\n",
    "    print(\"response:\", response)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='มีHDMI กี่ช่อง', additional_kwargs={}, example=False),\n",
       " AIMessage(content='The SONY KD-50X75K TV has 3 HDMI ports.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='มี HDMI กี่ช่อง', additional_kwargs={}, example=False),\n",
       " AIMessage(content='The SONY KD-50X75K LED TV has 3 HDMI ports.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(executor.memory.buffer)\n",
    "executor.memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_memory': ChatMessageHistory(messages=[HumanMessage(content='มีความจุเท่าไหร่ฦ', additional_kwargs={}, example=False), AIMessage(content='ความจุของสินค้าคือ 15 กิโลกรัม', additional_kwargs={}, example=False), HumanMessage(content='มีความจุเท่าไหร่ฦ', additional_kwargs={}, example=False), AIMessage(content='ความจุของสินค้าคือ 15 กิโลกรัม', additional_kwargs={}, example=False)]),\n",
       " 'output_key': None,\n",
       " 'input_key': None,\n",
       " 'return_messages': True,\n",
       " 'human_prefix': 'Human',\n",
       " 'ai_prefix': 'AI',\n",
       " 'memory_key': 'chat_history',\n",
       " 'k': 5}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.memory.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "executor.memory.chat_memory.add_user_message(\"You are a ping pong machine\")\n",
    "executor.memory.chat_memory.add_ai_message(\"ping pong machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='มีความจุเท่าไหร่ฦ', additional_kwargs={}, example=False),\n",
       " AIMessage(content='ความจุของสินค้าคือ 15 กิโลกรัม', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='มีความจุเท่าไหร่ฦ', additional_kwargs={}, example=False),\n",
       " AIMessage(content='ความจุของสินค้าคือ 15 กิโลกรัม', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='You are a ping pong machine', additional_kwargs={}, example=False),\n",
       " AIMessage(content='ping pong machine', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.agents.agent.AgentExecutor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.agents.agent.AgentExecutor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='\\n        You are a Product information chatbot name naxon( first letter n from neuron and axon from axon). \\n        You can ask questions to help you understand user intent.\\n        You should only talk within the context of problem.\\n        If you are unsure of how to help, you can suggest the client to contact the customer support.\\n        You should talk on Thai, unless the client talks in English. \\n        ', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history'),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> qa-product-information: Useful when you need to information about the product.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\ The action to take. Must be one of qa-product-information\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.agent.__dict__['llm_chain'].prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"qa-product-information\",\n",
      "    \"action_input\": \"spell 'bee'\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mB-E-E\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"B-E-E\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'B-E-E'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"spell 'bee'\"\n",
    "try:\n",
    "    response= executor.run(message)\n",
    "except Exception as e:\n",
    "        response = str(e)\n",
    "        if response.startswith(\"Could not parse LLM output:\"):\n",
    "            response = response.removeprefix(\"Could not parse LLM output:\").removesuffix(\"`\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The word 'bee' has three letters.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The word 'bee' has three letters.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"how many letter?\"\n",
    "try:\n",
    "    response= executor.run(message)\n",
    "except Exception as e:\n",
    "        response = str(e)\n",
    "        if response.startswith(\"Could not parse LLM output:\"):\n",
    "            response = response.removeprefix(\"Could not parse LLM output:\").removesuffix(\"`\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, but as an AI chatbot, I don't have access to personal information. Therefore, I don't know your name. Is there anything else I can assist you with?\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"what is my name?\"\n",
    "try:\n",
    "    response= executor.run(message)\n",
    "except Exception as e:\n",
    "        response = str(e)\n",
    "        if response.startswith(\"Could not parse LLM output:\"):\n",
    "            response = response.removeprefix(\"Could not parse LLM output:\").removesuffix(\"`\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse LLM output: I'm sorry, but as an AI chatbot, I don't have access to personal information. Therefore, I don't know your name. Is there anything else I can assist you with?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/agents/conversational_chat/output_parser.py:28\u001b[0m, in \u001b[0;36mConvoOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[39m# Attempt to parse the text into a structured format (assumed to be JSON\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m# stored as markdown)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     response \u001b[39m=\u001b[39m parse_json_markdown(text)\n\u001b[1;32m     30\u001b[0m     \u001b[39m# If the response contains an 'action' and 'action_input'\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/output_parsers/json.py:68\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m parsed \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(json_str)\n\u001b[1;32m     70\u001b[0m \u001b[39mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwhat is my name?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m response\u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39;49mrun(message)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:475\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    474\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    476\u001b[0m         _output_key\n\u001b[1;32m    477\u001b[0m     ]\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    481\u001b[0m         _output_key\n\u001b[1;32m    482\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:282\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    283\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    284\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/chains/base.py:276\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    270\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    271\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    277\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    278\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/agents/agent.py:1036\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1036\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m   1037\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1038\u001b[0m         color_mapping,\n\u001b[1;32m   1039\u001b[0m         inputs,\n\u001b[1;32m   1040\u001b[0m         intermediate_steps,\n\u001b[1;32m   1041\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1042\u001b[0m     )\n\u001b[1;32m   1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1044\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1045\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1046\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/agents/agent.py:844\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    842\u001b[0m     raise_error \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[39mif\u001b[39;00m raise_error:\n\u001b[0;32m--> 844\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    845\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m    846\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/agents/agent.py:833\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    830\u001b[0m     intermediate_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m    832\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 833\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m    834\u001b[0m         intermediate_steps,\n\u001b[1;32m    835\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    836\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m    837\u001b[0m     )\n\u001b[1;32m    838\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    839\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/agents/agent.py:457\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m full_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    456\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 457\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(full_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatgpt/lib/python3.10/site-packages/langchain/agents/conversational_chat/output_parser.py:50\u001b[0m, in \u001b[0;36mConvoOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[39mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     45\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing \u001b[39m\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39maction_input\u001b[39m\u001b[39m'\u001b[39m\u001b[39m in LLM output: \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m         )\n\u001b[1;32m     47\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     48\u001b[0m     \u001b[39m# If any other exception is raised during parsing, also raise an\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[39m# OutputParserException\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse LLM output: \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: I'm sorry, but as an AI chatbot, I don't have access to personal information. Therefore, I don't know your name. Is there anything else I can assist you with?"
     ]
    }
   ],
   "source": [
    "# message = \"what is my name?\"\n",
    "# response= executor.run(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I am Naxon, a product information chatbot. I am here to assist you with any questions you have about products. How can I help you today?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"who r u?\"\n",
    "try:\n",
    "    response= executor.run(message)\n",
    "except Exception as e:\n",
    "        response = str(e)\n",
    "        if response.startswith(\"Could not parse LLM output:\"):\n",
    "            response = response.removeprefix(\"Could not parse LLM output:\").removesuffix(\"`\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' ผมคือแชทบอทชื่อ Naxon ที่มาเพื่อช่วยให้ข้อมูลเกี่ยวกับผลิตภัณฑ์ มีอะไรที่ผมสามารถช่วยเพิ่มเติมได้หรือไม่ครับ?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"คุณคือใคร\"\n",
    "try:\n",
    "    response= executor.run(message)\n",
    "except Exception as e:\n",
    "        response = str(e)\n",
    "        if response.startswith(\"Could not parse LLM output:\"):\n",
    "            response = response.removeprefix(\"Could not parse LLM output:\").removesuffix(\"`\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' ขอโทษครับ ฉันไม่สามารถรู้ชื่อของคุณได้ เนื่องจากฉันเป็นแค่แชทบอทที่ให้ข้อมูลเกี่ยวกับผลิตภัณฑ์เท่านั้น ถ้าคุณมีคำถามหรือข้อสงสัยใด ๆ เกี่ยวกับผลิตภัณฑ์ ฉันยินดีที่จะช่วยเสมอครับ'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is the dimension of washing machine?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='The dimensions of the washing machine are 848 x 596.5 x 575 mm (Height / Width / Depth).', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='what did I just say?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='You asked about the dimensions of the washing machine.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='my name is Ford', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Nice to meet you, Ford! How can I assist you today?', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='what did I just say?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"You asked, 'what did I just say?'\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='my name is Ford', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Nice to meet you, Ford! How can I assist you today?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
