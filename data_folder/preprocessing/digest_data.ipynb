{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process steps\n",
    "\n",
    "1. send pdf base64\n",
    "1. convert pdf base64 to pils\n",
    "1. do ocr\n",
    "1. sort line\n",
    "1. translate text to english\n",
    "1. do vector\n",
    "1. start chat\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. send pdf base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../db_folder/product_a'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"../upload_folder/product_a/electrolux_washing_machine.pdf\"\n",
    "# file_path = \"../data_folder/product_b/lg_washing_machine.pdf\"\n",
    "# file_path = \"../data_folder/product_a_small/electrolux_washing_machine.pdf\"\n",
    "\n",
    "# load the pdf to image\n",
    "folder_path = os.path.dirname(file_path)\n",
    "preprocessed_data_folder = folder_path.replace(\"upload_folder\", \"preprocessed_data\")\n",
    "# product id is folder_path folder name\n",
    "\n",
    "product_id = os.path.basename(folder_path)\n",
    "persist_directory = f\"../db_folder/{product_id}\"\n",
    "persist_directory\n",
    "# create folder if not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "\n",
    "os.makedirs(preprocessed_data_folder, exist_ok=True)\n",
    "\n",
    "image_name_list = []\n",
    "\n",
    "\n",
    "def pdf_save_text_each_page(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            # Read all pages and extract text\n",
    "            for ind, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                # save text to image_{ind}.txt\n",
    "                image_name = f\"image_{ind}.txt\"\n",
    "                image_name_list.append(image_name)\n",
    "                file_path = os.path.join(preprocessed_data_folder, image_name)\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    f.write(text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error reading PDF:\", e)\n",
    "\n",
    "\n",
    "pdf_save_text_each_page(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_0.txt', 'image_1.txt', 'image_2.txt']\n",
      "['image_37.txt', 'image_38.txt', 'image_39.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_name_list = os.listdir(preprocessed_data_folder)\n",
    "# .txt only\n",
    "\n",
    "image_name_list = [i for i in image_name_list if i.endswith(\".txt\")]\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_numeric_part(filename):\n",
    "    # Use regular expression to extract the numeric part of the filename\n",
    "    match = re.search(r\"\\d+\", filename)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "\n",
    "# Sort the file names based on the numeric part\n",
    "image_name_list = sorted(image_name_list, key=extract_numeric_part)\n",
    "\n",
    "print(image_name_list[:3])\n",
    "print(image_name_list[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_0.txt', 'image_1.txt', 'image_2.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_list = []\n",
    "text_list = []\n",
    "\n",
    "for ind, image_name in enumerate(image_name_list):\n",
    "    text_path = os.path.join(\n",
    "        preprocessed_data_folder, image_name.split(\".\")[0] + \".txt\"\n",
    "    )\n",
    "\n",
    "    with open(text_path, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    meta_data = {\n",
    "        \"page_index\": ind + 1,\n",
    "        \"data_name\": file_path.split(\"/\")[-1],\n",
    "    }\n",
    "    # meta_data = {\"hellow\": ind+1}\n",
    "    text_list.append(text)\n",
    "    meta_data_list.append(meta_data)\n",
    "# Step 3: Create function to count tokens\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=300,\n",
    "    # chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=count_tokens,\n",
    ")\n",
    "chunks = text_splitter.create_documents(text_list, meta_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "credential_path = \"../.credential\"\n",
    "load_dotenv(credential_path)\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "import time\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = None\n",
    "try:\n",
    "    assert os.path.exists(persist_directory) == True\n",
    "\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    print(\"Load chroma index success\")\n",
    "except:\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "    # for ind, chunk in enumerate(chunks):\n",
    "    #     if db is None:\n",
    "    #         print(\"create new chroma index success\")\n",
    "    #         db = Chroma.from_documents(\n",
    "    #             documents=[chunk],\n",
    "    #             embedding=embeddings,\n",
    "    #             persist_directory=persist_directory,\n",
    "    #         )\n",
    "    #     else:\n",
    "    #         db.add_documents([chunk])\n",
    "    #         time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = db.get()\n",
    "data[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45ee3f46-3cc4-11ee-b2d6-acde48001122 1 electrolux_washing_machine.pdf\n"
     ]
    }
   ],
   "source": [
    "for id, doc, meta in zip(data[\"ids\"], data[\"documents\"], data[\"metadatas\"]):\n",
    "    print(id, meta[\"page_index\"], meta[\"data_name\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"question\": \"The maximum capacity of a washing machine is the largest amount of laundry it can hold.\",\n",
    "    \"chat_history\": [\n",
    "        (\n",
    "            \"The dimension of the EWF9024P5WB model is not specified.\",\n",
    "            \"The dimensions of the EWF9024P5WB model are 848 x 596.5 x 659 mm (Height / Width / Depth).\",\n",
    "        ),\n",
    "        (\n",
    "            \"The dimension of the EWF9024P5WB model is not specified.\",\n",
    "            \"The dimensions of the EWF9024P5WB model are 848 x 596.5 x 659 mm (Height / Width / Depth).\",\n",
    "        ),\n",
    "        (\n",
    "            \"The dimension of the EWF9024P5WB model is not specified.\",\n",
    "            \"The dimensions of the EWF9024P5WB model are 848 x 596.5 x 659 mm (Height / Width / Depth).\",\n",
    "        ),\n",
    "        (\n",
    "            \"The maximum capacity of a washing machine is the largest amount of laundry it can hold.\",\n",
    "            \"The maximum capacity of the washing machine depends on the model. For the EWF8024P5WB and EWF8024P5SB models, the maximum capacity is 4 kg. For the EWF9024P5WB model, the maximum capacity is 5.5 kg.\",\n",
    "        ),\n",
    "    ],\n",
    "    \"answer\": \"The maximum capacity of the washing machine depends on the model. For the EWF8024P5WB and EWF8024P5SB models, the maximum capacity is 4 kg. For the EWF9024P5WB model, the maximum capacity is 5.5 kg.\",\n",
    "    \"source_documents\": \"source\",\n",
    "    \"generated_question\": \"What is the maximum capacity of a washing machine?\",\n",
    "}\n",
    "# result ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
